---
date: 2025-02-28
tags:
  - daily-notes
  - median-shape
---
# The Multiscale Flat Norm

For a more detailed look at the flat norm see: [[Currents and the Flat Norm]].

We define an $m$-current to be an element of the dual space of smooth $m$-forms with compact support That is, if $\Omega_c^m(\R^n)$ is the space of smooth $m$-forms with compact support than an $m$-current $T$ is a linear functional
$$
	T:\Omega_c^m(\R^n) \rightarrow \R.
$$
One may view an $m$-current as a generalization of oriented subsurfaces. Indeed, given a compact rectifiable oriented set $M$ of dimension $m$, we can construct an $m$-current $R$ given by
$$
	R(\omega) = \int_M\omega.
$$
If the boundary $\partial M$ is also rectifiable, then it follows by Stokes' theorem that
$$
	\int_{\partial M}\omega = \int_Md\omega = R(d\omega).
$$
Hence, we define the boundary of an $m$-current $T$ to be the $(m-1)$-current $\partial T$ that satisfies
$$
	\partial T(\omega) = T(d\omega)
$$
for all $m$-forms $\omega$.

Given an $m$-current $T$, we define the mass of $T$ to be given by
$$
	M(T) = \sup\{T(\omega):\sup_x\|\omega(x)\|\le1\}.
$$
As we are viewing currents are oriented surfaces, one may view the mass of a current as it's $m$-dimensional volume.

> [!def] Flat Norm
> Given an $m$-current $T$, the **flat norm** is given by
> $$\bb{F}(T) = \inf\{M(T-\partial S)+M(S):\text{S is a (m+1)-current}\} $$

As an example, consider the figure below:

![[SS_2025-02-27_1740686141.png#invert | center]]

We decompose our 1-current $T$ into a 2-current $S$ and a 1-current $T-\partial S$ such that $M(T-\partial S)+M(S)$ is minimal.

By scaling the affect the mass of $S$ has, we get the multiscale flat norm

> [!def] Multiscale Flat Norm
> Given an $m$-current and $\lambda\ge0$, the **multiscale flat norm** is given by
> $$ \bb{F}_\lambda(T) = \inf\{M(T-\partial S) + \lambda M(S):S\text{ is a (m+1)-current}\}.$$

# The Median Shape

Given a dataset $x_1,x_2,\dots,x_n\in\R$, one typically finds the median by sorting the data and picking the middle value. However, one can find the median without sorting:
$$
	\hat{x} = \operatorname*{argmin}_{x\in\R}\sum_{i=1}^n|x_i-x|.
$$
This is known as the geometric median and generalizes to any metric space.

Under the multiscale flat norm, the space of $m$-currents becomes a metric space. Thus, given input $m$-currents $T_1,T_2,\dots,T_n$, we define the median current to be the $m$-current $\hat{T}$ given by
$$
	\hat{T} = \operatorname*{argmin}_{T}\sum_{i=1}^n\bb{F}_\lambda(T-T_i).
$$
Keeping with our viewing currents as shapes, we get that $\hat{T}$ is the **median shape**.

One way to generalize this notion is with the **mass regularized median shape**:
$$
	\hat{T} = \operatorname*{argmin}_{T}\sum_{i=1}^n\bb{F}_\lambda(T-T_i) + \mu M(T).
$$
for some $\mu\ge0$. In this generalized form, we are applying a penalizing cost given by the mass of the median shape. Thus, we are now looking for the central shape with smallest mass.

# Flat Norm on Simplicial Complexes

When it comes to actually computing the median shape, we need to approximate the underlying surface with simplicial complexes. Since a $p$-current $T$ can be viewed as a subsurface of a manifold $M$, we approximate $T$ by a $p$-chain $t$ living in a simplicial complex $K$ that is homotopic to $M$. In this view, our decomposition
$$
	T = (T-\partial S) + S
$$
can be viewed 
$$
	t = (t-\partial_{p+1} s) + s
$$
where $t$ is a $p$-chain and $s$ is a $(p+1)$-chain. Thus, we approximate the multiscale flat norm by
$$
	\bb{F}_\lambda(T) \approx \min\{V_p(t-\partial s)+\lambda V_{p+1}(s):s\in C_{p+1}\}
$$
where $V_p(t)$ is the $p$-dimensional volume.

Let $\sigma_1,\dots,\sigma_m$ and $\tau_1,\dots,\tau_n$ be the $p$ and $(p+1)$-simplices of $K$ respectively. Then a $p$-chain $t$ and a $(p+1)$-chain $s$ can be viewed as the sums
$$
	t = \sum_it_i\sigma_i~\text{ and }~s=\sum_js_j\tau_j
$$
where $t_i,s_j\in\Z$. Then the multiscale flat norm of a chain $t\in C_p$ is given by
$$
	\bb{F}_\lambda(t) = \min_{x\in C_p,s\in C_{p+1}}\left\{\sum_{i=1}^mV_p(\sigma_i)|x_i| + \lambda\sum_{j=1}^nV_{p+1}(\tau_j)|s_j|:x=t-\partial_{p+1} s\right\}.
$$

# Median Shape IP

Given a set of $p$-chains $t_1,t_2,\dots,t_N\in C_p$ we want to find the median chain $\hat{t}$. Using the geometric median as before we get that
$$
	\hat{t}=\operatorname*{argmin}_{t\in C_p}\sum_{h=1}^N\bb{F}_\lambda(t-t_h).
$$
We can solve this problem with an integer program. For each $t-t_h$, denote by $r_h\in C_p$ and $s_h\in C_{p+1}$ the flat norm decomposition, i.e., $t-t_h=r_h+\partial_{p+1}s$. Then, denoting the $i$-th component of $r_h$ by $r_{hi}$ gives the following optimization problem:
$$
\begin{align*}
	\text{minimize } &&\sum_{h=1}^N\left(\sum_{i=1}^mV_p(\sigma_i)|r_{hi}|+\lambda\sum_{j=1}^nV_{p+1}(\tau_j)|s_{hj}|\right) \\
	\text{subject to } && t - t_h = r_h + \partial_{p+1}s && h=1,2,\dots, N \\
	&& t\in\Z^m,~r_h\in\Z^m,~s_h\in\Z_m && h=1,2,\dots,N.
\end{align*}
$$
We want to convert this into an integer program. First notice, that the boundary $\partial_{p+1}s_h$ can be viewed as a matrix product $Bs_h$ where $B$ is the $(p+1)$-boundary matrix of $K$. Next, we correct the $|r_{hi}|$ and $|s_{hj}|$ terms by replacing them with
$$
	r_h = r_h^+ - r_h^-~\text{ and }~s_h=s_h^+-s_h^-.
$$
Thus, our problem becomes 
$$
\begin{align*}
	\text{minimize } &&\sum_{h=1}^N\left(\sum_{i=1}^mV_p(\sigma_i)(r_{hi}^++r_{hi}^-)+\lambda\sum_{j=1}^nV_{p+1}(\tau_j)(s_{hj}^++s_{hj}^-)\right) \\
	\text{subject to } && t - t_h = (r_h^+-r_h^-) + B(s_h^+-s_h^-) && h=1,2,\dots, N \\
	&& r_h^+,r_h^-\ge0,~ s_h^+,s_h^-\ge0 && h=1,2,\dots,N \\
	&& t\in\Z^m,~r_h^+,r_h^-\in\Z^m,~s_h^+,s_h^-\in\Z_m && h=1,2,\dots,N.
\end{align*}
$$
which is now an IP.

## Generalized Median Shape IP

To convert our IP into the mass regularized median shape, we need to add the mass of $T$ to our objective function. That is, we add
$$
	\mu\sum_{i=1}^n V_p(\sigma_i)|t_i|
$$
which we can convert to be linear by writing $t=t^+-t^-$. We also add weights $\alpha_1,\dots,\alpha_N\ge0$ to each input chain where $\sum\alpha_h=1$. These changes give the generalized median shape IP:
$$
\begin{align*}
	\text{minimize } &&\mu\sum_{i=1}^nV_p(\sigma_i)(t_i^++t_i^-) + \sum_{h=1}^N\alpha_h\left(\sum_{i=1}^mV_p(\sigma_i)(r_{hi}^++r_{hi}^-)+\lambda\sum_{j=1}^nV_{p+1}(\tau_j)(s_{hj}^++s_{hj}^-)\right) \\
	\text{subject to } && t^+ -t^- - t_h = (r_h^+-r_h^-) + B(s_h^+-s_h^-) && h=1,2,\dots, N \\
	&& t^+,t^-\ge0,~r_h^+,r_h^-\ge0,~ s_h^+,s_h^-\ge0 && h=1,2,\dots,N \\
	&& t^+,t^-\in\Z^m,~r_h^+,r_h^-\in\Z^m,~s_h^+,s_h^-\in\Z_m && h=1,2,\dots,N.
\end{align*}
$$

When solving the generalized median shape IP, we solve the relaxed version by dropping the integral requirements.
# Total Unimodularity and Integrality

Writing our linear program as a matrix gives the result of 
$$
\begin{align*}
	\min & \Big[\begin{array}
		1\mu\begin{bmatrix} w & w \end{bmatrix} & \alpha_1\begin{bmatrix}w & w & \lambda v & \lambda v\end{bmatrix} & \alpha_2\begin{bmatrix}w & w & \lambda v & \lambda v\end{bmatrix} & \cdots & \alpha_N\begin{bmatrix}w & w & \lambda v & \lambda v\end{bmatrix}
	\end{array}\Big]x \\
	\text{s.t.} & \\
	& \left[\begin{array}
		1\begin{bmatrix} I & -I \end{bmatrix} & \begin{bmatrix} -I & I & -B & B \end{bmatrix} \\
		\begin{bmatrix} I & -I \end{bmatrix} & & \begin{bmatrix} -I & I & -B & B \end{bmatrix} \\
		\vdots & & & \ddots \\
		\begin{bmatrix} I & -I \end{bmatrix} & &&& \begin{bmatrix} -I & I & -B & B \end{bmatrix} \\
	\end{array}\right]x = 
	\begin{bmatrix}t_1 \\ t_2 \\ \vdots \\ t_N\end{bmatrix} \\\\
	& \begin{bmatrix}
		t^+ & t^- & r_1^+ & r_1^- & s_1^+ & s_1^- & r_2^+ & r_2^- & s_2^+ & s_2^- & \cdots & & r_N^+ & r_N^- & s_N^+ & s_N^-
	\end{bmatrix} \ge 0
\end{align*}
$$
where $w=[V_p(\sigma_i)]_{i=1}^m$ and $v=[V_{p+1}(\tau_j)]_{j=1}^n$.

> [!def] Totally Unimodular Matrix
> A matrix $A$ is **totally unimodular** (TU) if every square submatrix has determinant 0, 1 or -1.

TU matrices are crucial in integer optimization. Consider the following theorem by Hoffman and Kruskal (1956):

> [!thm]
> Let $A\in\Z^{m\times n}$. Then the polyhedra $P=\{x\mid Ax\le b,~x\ge0\}$ is integral for all $b\in\Z^m$ in which $P\ne\emptyset$ if and only if $A$ is totally unimodular.

Since we are looking for integral solutions, it suffices to show that our matrix $A$ is totally unimodular. Typically one would do this by constructing $A$ with a series of operations that preserve TU. Such operations include, but are not limited to,
- Permuting rows or columns.
- Taking the transpose.
- Multiplying a row or column by -1.
- Adding a row or column of all zeros, or adding a row or column with one nonzero that is $\pm1$.
- Repeating a row or column.

We can construct our matrix $A$ with the following procedure:
1. Starting with $B$ form $\begin{bmatrix}-I & I & -B & B\end{bmatrix}$.
2. Take the $N$-fold $I$-sum of the matrix $\begin{bmatrix}-I & I & -B & B\end{bmatrix}$.
3. Take the transpose.
4. Copy the first $m$ columns and negate to add a column of $-I$.

While steps 1, 3 and 4 preserve TU, the $N$-fold $I$-sum is not guaranteed to preserve TU. Despite this, all experiments have provided integral solutions.

# Open Questions

- Are boundary matrices TU?
- Is $A$ in our linear program TU?
- Solving our LP via the simplex algorithm requires one to check all $2^{4N+2}$ vertices in the worst case scenario. Can we construct a more efficient algorithm that has a polynomial complexity?
