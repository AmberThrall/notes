---
date: 2024-07-24
tags:
  - an-introduction-to-manifolds
---
# Dual Space

Let $V$ be a vector space. We define the **dual space** of $V$ by
$$
	V^\vee = \Hom(V,\R).
$$
We call elements of $V^\vee$ **1-covectors** on $V$.

We assume that $V$ is finite-dimensional with basis $e_1,\dots,e_n$, i.e., for all $v\in V$ there is a unique linear combination $v=\sum v^ie_i$ with $v^i\in\R$. Define $\alpha^i:V\rightarrow\R$ by
$$
	\alpha^i(e_j) = \delta_j^i = \begin{cases}
		1 & \text{if }i = j\\
		0 & \text{if }i\ne j.
	\end{cases}
$$
> [!prp]
> The functions $\alpha^1,\dots,\alpha^n$ form a basis for $V^\vee$.

**Proof:** We need to show they span and are linearly independent. Let $f\in V^\vee$ and $v\in V$. Notice that
$$
	f(v) = \sum v^if(e_i) = \sum f(e_i)\alpha^i(v).
$$
So
$$
	f = \sum f(e_i)\alpha^i.
$$
Thus, $\alpha^1,\dots,\alpha^n$ spans $V^\vee$. Next let $v^1,\dots,v^n\in\R$ such that
$$
	\sum v^i\alpha^i = 0.
$$
Then it follows that
$$
	\sum v^i\alpha^i(e_j) = v^j = 0
$$
for all $j=1,\dots,n$. Therefore, $v=0$.
<p style='text-align:right'>Q.E.D.</p>

> [!cor]
> The dual space $V^\vee$ has the same dimension as $V$.

# Permutations

A **permutation** of a set $A=\{1,\dots,k\}$ is a bijection $\sigma:A\rightarrow A$. It is common to visualize permutations as matrices. For example, the permutation
$$
	\begin{bmatrix}
		1 & 2 & 3 & 4 & 5 \\
		2 & 4 & 5 & 1 & 3
	\end{bmatrix}
$$
maps $1\mapsto 2$, $2\mapsto 4$, $3\mapsto 5$, $4\mapsto 1$ and $5\mapsto 3$.

The product of two permutations $\tau$ and $\sigma$ of $A$ is the permutation $\tau\circ\sigma:A\rightarrow A$ given by composition. The **cyclic permutation**, $(a_1~a_2~\cdots~a_r)$, where each $a_i$ is distinct, is the permutation $\sigma$ such that $\sigma(a_1)=a_2$, $\sigma(a_2)=a_3$, ..., $\sigma(a_{r-1})=a_r$ and $\sigma(a_r)=a_1$. Any permutation can be written as a product of disjoint transpositions. For example, 
$$
	(1~2~3~4~5) = (1~5)(1~4)(1~3)(1~2).
$$

We define $S_k$ to be the group of all permutations on the set $\{1,\dots,k\}$ with the group operation defined by composition. A permutation $\sigma\in S_k$ is either **even** or **odd** depending on the number of transpositions in $\sigma$'s decomposition. We define the **sign** of $\sigma$ as +1 if it is even, or -1 if it is odd. One may show that
$$
	\textup{sgn}(\sigma\tau) = \textup{sgn}(\sigma)\textup{sgn}(\tau)
$$
for all $\sigma,\tau\in S_k$.

An **inversion** in a permutation $\sigma$ is an ordered pair $(\sigma(i),\sigma(j))$ such that $i < j$ but $\sigma(i)>\sigma(j)$. For example, the permutation
$$
	\begin{bmatrix}
		1 & 2 & 3 & 4 & 5 \\
		2 & 4 & 5 & 1 & 3
	\end{bmatrix}.
$$
has inversions $(2,1)$, $(4,1)$, $(5,1)$, $(4,3)$ and $(5,3)$.

> [!prp]
> A permutation is even if and only if it has an even number of inversions.

# Multilinear Functions

A function $f:V^k=V\times\cdots\times V\rightarrow\R$ is **$k$-linear** if it linear in each of its arguments:
$$
	f(\dots,av+bw,\dots) = af(\dots,v,\dots) + bf(\dots,w,\dots)
$$
A $k$-linear function is also called a **$k$-tensor. 

**Example:** With respect to the standard basis $e_1,\dots,e_n$ for $\R^n$, the dot product
$$
	f(v,w) = v\cdot w = \sum v^iw^i
$$
is bilinear.

> [!def]
> A $k$-linear function $f:V^k\rightarrow\R$ is **symmetric** if
> $$
> 	f(v_{\sigma(1)},\dots,v_{\sigma(k)}) = f(v_1,\dots,v_k)
> $$
> for all permutations $\sigma\in S_k$; it is **alternating** if
> $$
> 	f(v_{\sigma(1)},\dots,v_{\sigma(k)}) = (\textup{sgn}(\sigma))f(v_1,\dots,v_k).
> $$

We define the space $A_k(V)$ to be all alternating $k$-linear functions on a vector space $V$. We call such functions **alternating k-tensors** or **$k$-covectors**.

# The Permutation Action on Multilinear Functions

If $G$ is a group and $X$ is a set, the map $G\times X\mapsto X$ defined by 
$$
	(\sigma,x)\mapsto \sigma x
$$
is called a **left action** of $G$ on $X$ if
1. $e\cdot x=x$ for all $x\in X$
2. $\tau\cdot(\sigma\cdot x)=(\tau\sigma)\cdot x$ for all $\tau,\sigma\in G$ and $x\in X$.
A **right action** of $G$ on $X$ is defined similarily.

The **orbit** of an element $x\in X$ is the set
$$
	Gx := \{\sigma\cdot x\mid \sigma\in G\}
$$
As we saw in the definition above, we can define a group action on $k$-linear functions by $S_k$ by defining
$$
	(\sigma f)(v_1,\dots,v_k) = f(v_{\sigma(1)},\dots,v_{\sigma(k)}).
$$
Thus, $f$ is symmetric if and only if $\sigma f=f$ and alternating if and only if $\sigma f=(\textup{sgn}(\sigma))f$.

In the case $k=1$, $S_1$ is the trivial group and all 1-linear functions are both symmetric and alternating.

>[!lemma]
>If $\sigma,\tau\in S_k$ and $f$ is a $k$-linear function on $V$, then $\tau(\sigma f)=(\tau\sigma)f$.

**Proof:** Notice that
$$
\begin{align*}
	\tau(\sigma f)(v_1,\dots,v_k) &= \tau f(v_{\sigma(1)},\dots,v_{\sigma(k)}) \\
	&= f(v_{\tau(\sigma(1))},\dots,v_{\tau(\sigma(k))}) \\
	&= f(v_{\tau\sigma(1)},\dots,v_{\tau\sigma(k)}) \\
	&= (\tau\sigma)f(v_1,\dots,v_k).
\end{align*}
$$
<p style='text-align: right'>Q.E.D.</p>

# The Symmetrizing and Alternating Operators

From any $k$-linear function $f$ on a vector space $V$, we can construct a symmetric $k$-linear function $Sf$ by adding all possible permutations:
$$
	Sf = \sum_{\sigma\in S_k}\sigma f.
$$
Indeed, notice that for any $\tau\in S_k$,
$$
\begin{align*}
	\tau(Sf) &= \sum_{\sigma\in S_k}\tau(\sigma f) \\
	&= \sum_{\sigma\in S_k}(\tau\sigma)f \\
	&= \sum_{\sigma'\in S_k}\sigma' f \\
	&= Sf.
\end{align*}
$$

Similarly, we can construct an alternating $k$-linear function
$$
	Af = \sum_{\sigma\in S_k}(\textup{sgn}~\sigma)\sigma f.
$$
Notice that for any $\tau\in S_k$,
$$
\begin{align*}
	\tau(Af) &= \sum_{\sigma\in S_k}(\textup{sgn}~\sigma)\tau(\sigma f) \\
	&= \sum_{\sigma\in S_k}(\textup{sgn}~\sigma)(\tau\sigma) f \\
	&= (\textup{sgn}~\tau)\sum_{\sigma\in S_k}(\textup{sgn}~\tau\sigma)(\tau\sigma) f \\
	&= (\textup{sgn}~\tau)\sum_{\sigma'\in S_k}(\textup{sgn}~\sigma')\sigma' f \\
	&= (\textup{sgn}~\tau)Af.
\end{align*}
$$
> [!lemma]
> If $f$ is an alternating $k$-linear function on a vector space $V$, then $Af=(k!)f$.

**Proof:** Notice that $\sigma f=(\textup{sgn}~\sigma)f$ and $\textup{sgn}~\sigma\in\{-1,1\}$. Thus,
$$
	Af = \sum_{\sigma\in S_k}(\textup{sgn}~\sigma)\sigma f = \sum_{\sigma\in S_k}(\textup{sgn}~\sigma)^2f = \sum_{\sigma\in S_k}f = (k!)f.
$$
<p style='text-align: right'>Q.E.D.</p>

# The Tensor Product

Let $f$ be a $k$-linear function and $g$ an $\ell$-linear function. Then their **tensor product** is the $(k+\ell)$-linear function defined by
$$
	(f\otimes g)(v_1,\dots,v_{k+\ell}) = f(v_1,\dots,v_k)g(v_{k+1},\dots_{k+\ell}).
$$

**Example:** Let $e_1,\dots,e_n$ be the basis for $V$ and $\alpha^1,\dots,\alpha^n$ the basis for $V^\vee$ and $\langle\cdot,\cdot\rangle:V\times V\rightarrow\R$ a bilinear map. So for $v=\sum v^ie_i$ and $w=\sum w^ie_i$ we get that
$$
	\langle v,w\rangle = \sum v^iw^j\langle e_i,e_j\rangle = \sum\alpha^i(v)\alpha^j(w)\langle e_i,e_j\rangle = \sum\langle e_i,e_j\rangle(\alpha^i\otimes\alpha^j)(v,w).
$$
Hence $\langle\cdot,\cdot\rangle=\sum\langle e_i,e_j\rangle\alpha^i\otimes\alpha^j$.

>[!prp]
>The tensor product is associative:
>$$
>	(f\otimes g)\otimes h = f\otimes(g\otimes h).
>$$

**Proof:** Let $f$ be $k$-linear, $g$ be $\ell$-linear and $h$ be $m$-linear. Notice that
$$
\begin{align*}
	((f\otimes g)\otimes h)(v_1,\dots,v_{k+\ell+m}) &= (f\otimes g)(v_1,\dots,v_{k+\ell})h(v_{k+\ell+1},\dots,v_{k+\ell+m}) \\
	&= f(v_1,\dots,v_k)g(v_{k+1},\dots,v_{k+\ell})h(v_{k+\ell+1},\dots,v_{k
	+\ell+m}) \\
	&= f(v_1,\dots,v_k)(g\otimes h)(v_{k+1},\dots,v_{k+\ell+m}) \\
	&= (f\otimes(g\otimes h))(v_1,\dots,v_{k+\ell+m}).
\end{align*}
$$
<p style='text-align: right'>Q.E.D.</p>

# The Wedge Product

Let $f\in A_k(V)$ be an alternating $k$-linear function and $g\in A_\ell(V)$ be an alternating $\ell$-linear function. The **wedge product** or **exterior product** $f\wedge g$ is the alternating $(k+\ell)$-linear function
$$
	f\wedge g = \frac{1}{k!\ell!}A(f\otimes g),
$$
i.e.,
$$
	(f\wedge g)(v_1,\dots,v_{k+\ell}) = \frac{1}{k!\ell!}\sum_{\sigma\in S_{k+\ell}}(\sgn~\sigma)f(v_
	{\sigma(1)},\dots,v_{\sigma(k)})g(v_{\sigma(k+1)},\dots,v_{\sigma(k+\ell)}).
$$

**Example:** For $f\in A_2(V)$ and $g\in A_1(V)$,
$$
\begin{align*}
	A(f\otimes g)(v_1,v_2,v_3) &= f(v_1,v_2)g(v_3) - f(v_1,v_3)g(v_2) + f(v_2,v_3)g(v_1) \\
	&\phantom{=} - f(v_2,v_1)g(v_3) + f(v_3,v_1)g(v_2) - f(v_3,v_2)g(v_1).
\end{align*}
$$
Since $f$ is alternating $f(v_1,v_2)=-f(v_2,v_1)$ and so on. So 
$$
	(f\wedge g)(v_1,v_2,v_3) = \frac{A(f\otimes g)(v_1,v_2,v_3) }{2} = f(v_1,v_2)g(v_3) - f(v_1,v_3)g(v_2) + f(v_2,v_3)g(v_1).
$$

To avoid duplicate terms in the sum, one may only consider $(k,\ell)$-shuffles where
$$
	\sigma(1)<\cdots<\sigma(k)~\text{ and }~\sigma(k+1)<\cdots<\sigma(k+\ell).
$$
This reduces the wedge product to a sum of $\binom{k+\ell}{k}$ terms instead of $(k+\ell)!$, i.e.,
$$
	(f\wedge g)(v_1,\dots,v_{k+\ell}) = \sum_{(k,\ell)\text{-shuffles}}(\textup{sgn}~\sigma)f(v_{\sigma(1),\dots,v_{\sigma(k)}})g(v_{\sigma_{k+1}},\dots,v_{\sigma(k+\ell)}).
$$

**Example:** If $f$ and $g$ are 1-covectors on $V$, then
$$
	(f\wedge g)(v_1,v_2) = f(v_1)g(v_2) - f(v_2)g(v_1).
$$

**Example:** For $f,g\in A_2(V)$,
$$
\begin{align*}
	(f\wedge g)(v_1,v_2,v_3,v_4) &= f(v_1,v_2)g(v_3,v_4) - f(v_1,v_3)g(v_2,v_4) + f(v_1,v_4)g(v_2,v_3) \\
	&\phantom{=}+f(v_2,v_3)g(v_1,v_4) - f(v_2,v_4)g(v_1,v_3) + f(v_3,v_4)g(v_1,v_2).
\end{align*}
$$

# Anticommutativity of the Wedge Product

> [!prp]
> The wedge prouct is anticommuatative: if $f\in A_k(V)$ and $g\in A_\ell(V)$, then
> $$
> 	f\wedge g = (-1)^{k\ell}g\wedge f.
> $$

**Proof:** Define $\tau\in S_{k+\ell}$ by
$$
\begin{bmatrix}
	1 & \cdots & \ell & \ell+1 & \cdots & \ell+k \\
	k+1 & \cdots & k+\ell & 1 & \cdots & k
\end{bmatrix}.
$$
Notice that
$$
	\sigma(1)=\sigma\tau(\ell+1),\dots,\sigma(k)=\sigma\tau(\ell+k)
$$
and
$$
	\sigma(k+1) = \sigma\tau(1),\dots,\sigma(k+\ell) = \sigma\tau(\ell).
$$
Thus,
$$
\begin{align*}
	A(f\otimes g)(v_1,\dots,v_{k+\ell}) &= \sum_{\sigma\in S_{k+\ell}}(\textup{sgn}~\sigma)f(v_{\sigma(1)},\dots,v_{\sigma(k)})g(v_{\sigma(k+1)},\dots,v_{\sigma(k+\ell)}) \\
	&= \sum_{\sigma\in S_{k+\ell}}(\textup{sgn}~\sigma)f(v_{\sigma\tau(\ell+1)},
	\dots,v_{\sigma\tau(\ell+k)})g(v_{\sigma\tau(1)},\dots,\sigma\tau(\ell)) \\	
	&= (\textup{sgn}~\tau)\sum_{\sigma\in S_{k+\ell}}(\textup{sgn}~\sigma\tau)f(v_{\sigma\tau(\ell+1)},
	\dots,v_{\sigma\tau(\ell+k)})g(v_{\sigma\tau(1)},\dots,\sigma\tau(\ell)) \\
	&= (\textup{sgn}~\tau)(g\otimes f)(v_1,\dots,v_{k+\ell}).
\end{align*}
$$
Dividing by $k!\ell!$ gives
$$
	f\wedge g = (\textup{sgn}~\tau)g\wedge f
$$
and $\textup{sgn}~\tau=(-1)^{k\ell}$.
<p style='text-align: right'>Q.E.D.</p>

> [!cor]
> If $f$ is a multicovector of odd degree on $V$, then $f\wedge f=0$.

**Proof:** Let $f$ be $k$-linear where $k$ is odd. Then
$$
	f\wedge f = (-1)^{k^2} f\wedge f = -f\wedge f.
$$
Therefore, $f\wedge f=0$.
<p style='text-align: right'>Q.E.D.</p>

# Associativity of the Wedge Product

> [!prp] 
> Let $V$ be a real vector space and $f,g,h$ alternating multilinear functions on $V$ of degrees $k,\ell,m$ respectively. Then
> $$
> 	(f\wedge g)\wedge h = f\wedge(g\wedge h).
> $$

As a result, one may show that 
$$
	f\wedge g\wedge h = \frac{1}{k!\ell!m!}A(f\otimes g\otimes h)
$$
which naturally extends to arbitrary many alternating multilinear functions $f_i\in A_{d_i}(V)$,
$$
	f_1\wedge\cdots\wedge f_r = \frac{1}{(d_1)!\cdots(d_r)!}A(f_1\otimes\cdots\otimes f_r).
$$

We denote by $[b_j^i]$ the matrix whose $(i,j)$-entry is $b_j^i$.

> [!prp]
> If $\alpha_1,\dots,\alpha^k$ are linear functions on a vector space $V$, then
> $$
> 	(\alpha^1\wedge\cdots\wedge\alpha^k)(v_1,\dots,v_k) = \det[\alpha^i(v_j)].
> $$

**Proof:** By [Leibniz formula](https://en.wikipedia.org/wiki/Leibniz_formula_for_determinants):
$$
\begin{align*}
	(\alpha^1\wedge\cdots\wedge\alpha^k)(v_1,\dots,v_k) &= \frac{1}{1!\cdots1!}A(\alpha^1\otimes\cdots\otimes\alpha^k)(v_1,\dots,v_k) \\
	&= \sum_{\sigma\in S_k}(\textup{sgn}~\sigma)\alpha^1(v_{\sigma(1)})\cdots\alpha^k(v_{\sigma(k)}) \\
	&= \det[\alpha^i(v_j)].
\end{align*}
$$
<p style='text-align: right'>Q.E.D.</p>

# A Basis for $k$-Covectors

We use the multi-index notation
$$
	I = (i_1,\dots,i_k)
$$
and write $e_I$ for $(e_{i_1},\dots,e_{i_k})$ and $\alpha^I$ for $\alpha^{i_1}\wedge\cdots\wedge\alpha^{i_k}$ where $e_1,\dots,e_n$ is the basis for $V$ and $\alpha^1,\dots,\alpha^n$ is the basis for $V^\vee$.

> [!lemma]
> Let $e_1,\dots,e_n$ be a basis for a vector space $V$ and $\alpha^1,\dots,\alpha^n$ its dual basis for $V^\vee$. If $I=(1\le i_1<\cdots<i_k\le n)$ and $J=(1\le j_1<\cdots<j_k\le n)$ are strictly ascending multi-indices, then
> $$
> 	\alpha^I(e_J) = (\alpha^{i_1}\wedge\cdots\wedge\alpha^{i_k})(e_{j_1},\dots,e_{j_k}) = \delta_j^I = \begin{cases}
> 		1 & \text{if }I = J\\
> 		0 & \text{if }I \ne J.
> 	\end{cases}
> $$

**Proof:** Notice that
$$
	\alpha^I(e_J) = \det[\alpha^i(e_j)]_{i\in I,j\in J}.
$$
If $I=J$, then  $[\alpha^i(e_j)]$ is the identity matrix. Assume that $I\ne J$ and let $\ell$ indicate the first terms that differ, i.e.,
$$
	i_1=j_1,\dots,i_{\ell-1}=j_{\ell-1}, i_\ell\ne j_\ell,\dots
$$
If $i_\ell< j_\ell$, then $i_\ell\ne j_1,\dots,i_\ell\ne j_{\ell-1}$ and $i_\ell\ne j_{l+1},\dots,i_\ell\ne j_{k}$. As a result, $\alpha^{i_\ell}(e_{j_1})=\cdots=\alpha^{i_\ell}(e_{j_k})=0$. Hence, the matrix has a zero row. A similar argument holds if $i_\ell > j_\ell$.
<p style='text-align: right'>Q.E.D.</p>

> [!prp]
> The alternating $k$-linear functions $\alpha^I$ for $I=(i_1<\cdots<i_k)$ form a basis for the space $A_k(V)$ of alternating $k$-linear functions on $V$.

**Proof:** We need to show spanning and linear independence. Assume that $\sum c_I\alpha^I=0$ where the sum runs across all possible strictly ascending multi-indices $I$. Notice then that
$$
	0 = \sum c_I\alpha^I(e_J) = c_I. 
$$
Thus, they are linearly independent.

Now let $f\in A_k(V)$. We claim that
$$
	f = \sum f(e_I)\alpha^I = g
$$
Notice that
$$
	g(e_J) = \sum f(e_I)\alpha^I(e_J) = f(e_J),
$$
thus $f$ and $g$ agree over the basis elements. Therefore, $f=g$.
<p style='text-align: right'>Q.E.D.</p>

> [!cor]
> If the vector space $V$ has dimension $n$, then $A_k(V)$ has dimension $\binom{n}{k}$.

> [!cor]
> If $k>\dim V$, then $A_k(V)=0$.

# Problems

##### 3.1. Tensor product of covectors
Let $e_1,\dots,e_n$ be a basis for a vector space $V$ and $\alpha^1,\dots,\alpha^n$ its dual basis in $V^\vee$. Let $[g_{ij}]\in\R^{n\times n}$ and define the bilinear function $f:V\times V\rightarrow\R$
$$
	f(v,w) = \sum_{1\le i,j\le n}g_{ij}v^iw^j
$$
for $v=\sum v^ie_i$ and $w=\sum w^ie_i$ in $V$. Describe $f$ in terms of the tensor products of $\alpha^i$ and $\alpha^j$, $1\le i,j\le n$.

**Answer:**
$$
\begin{align*}
	f(v,w) &= \sum_{1\le i,j\le n}g_{ij}\alpha^i(v)\alpha^j(w) = \sum_{1\le i,j\le n}g_{ij}(\alpha^i\otimes\alpha^j)(v,w)
\end{align*}
$$

##### 3.2. Hyperplanes
(a) Let $V$ be a vector space of dimension $n$ and $f:V\rightarrow\R$ a non-zero linear functional. Show that $\dim\textup{ker} f=n-1$, i.e., $\textup{ker}f$ is a hyperplane in $V$.

**Answer:** By rank-nullity,
$$
	\textup{rank}f + \textup{nullity}f = \textup{dim}V = n.
$$
Since $\textup{rank}f\le 1$ and $f$ is non-zero, it follows that $\textup{rank}f=1$.

(b) Show that a nonzero linear functional on a vector space $V$ is determined up to a multiplicative constant by its kernel, i.e., if $f,g:V\rightarrow\R$ are nonzero linear functionals and $\textup{ker} f=\textup{ker} g$ then $g=cf$ for some $c\in\R$.

**Answer:** Since $f$ is nonzero, there is some $v\in V$ such that $f(v)=r$ where $r\ne0$. We may assume WLOG that $r=1$, otherwise take $f(v/r)$. Notice that
$$
	f(x-f(x)v) = f(x) - f(x)f(v) = 0
$$
implies that $g(x-f(x)v)=0$. Thus,
$$
	g(x) = g(x-f(x)v)+g(f(x)v) = f(x)g(v).
$$

##### 3.4. A characterization of alternating $k$-tensors
Let $f$ be a $k$-tensor on a vector space $V$. Prove that $f$ is alternating if and only if $f$ changes sign whenever two successive arguments are interchanged:
$$
	f(\dots,v_{i+1},v_i,\dots) = -f(\dots,v_i,v_{i+1},\dots)
$$
for $i=1,\dots,k-1$.

**Answer:** Let $f$ be alternating. Notice that 
$$
	f(\dots,v_{i+1},v_i,\dots) = \textup{sgn}(i~i+1)f(\dots,v_i,v_{i+1},\dots) = -f(\dots,v_i,v_{i+1},\dots).
$$
For the converse, let $\sigma\in S_k$ and consider the decomposition of $\sigma$
$$
	\sigma=\tau_1\cdots\tau_m
$$
where each $\tau_i$ is a transposition. Notice that
$$
\begin{align*}
	f(v_1,\dots,v_k) &= -f(v_{\tau_1(1)},\dots,v_{\tau_1(k)}) \\
	&= f(v_{\tau_2\tau_1(1)},\dots,v_{\tau_2\tau_1(k)}) \\
	&\vdots \\
	&= (-1)^mf(v_{\sigma(1)},\dots,v_{\sigma(k)}) \\
	&= (\textup{sgn}~\sigma) f(v_{\sigma(1)},\dots,v_{\sigma(k)}).
\end{align*}
$$
Therefore, $f$ is alternating.

