---
date: 2024-07-23
---
# The Directional Derivative

One may visualize the tangent space $T_p(\R^n)$ at $p\in\R^n$ as the vector space of arrows emanating from $p$ tangentially. We write vectors $v\in T_p(\R^n)$ by $\langle v^1,\dots,v^n\rangle$. We denote the standard basis for $\R^n$ by $e_1,\dots, e_n$. Hence, $v=\sum v^ie_i$. We call vectors $v\in T_p(\R^n)$  **tangent vectors**. 

![[Tangentialvektor.svg.png#invert | center | 300]]

If $f$ is $C^\infty$ in a neighborhood of $p\in \R^n$ and $v\in T_p(\R^n)$, we define the **directional derivative** of $f$ in the direction of $v$ at $p$ by
$$
	D_vf = \lim_{t\rightarrow0}\frac{f(p+tv)-f(p)}{t} = \frac{d}{dt}\Bigg|_{t=0}f(p+tv).
$$
Notice that 
$$
	D_vf = v\cdot\grad f(p) = \sum_{i=1}^n v^i\frac{\partial f}{\partial x^i}(p).
$$
Note that $D_vf$ is a number, not a function. We write
$$
	D_v=\sum v^i\frac{\partial f}{\partial x^i}\Bigg|_p
$$
for the map that sends a function $f$ to $D_vf$.

# Germs of Functions

> [!def] Equivalence Relation
> A **relation** on a set $S$ is a subset $R\subset S\times S$. We say $x\sim y$ for $x,y\in S$ if $(x,y)\in R$. The relation $R$ is an **equivalence relation** if for all $x,y,z\in S$
> 1. $x\sim x$
> 2. if $x\sim y$, then $y\sim x$
> 3. If $x\sim y$ and $y\sim z$, then $x\sim z$.

Let $(f,U)$ where $U$ is a neighborhood of $p$ and $f:U\rightarrow\R$ is $C^\infty$. We say that $(f,U)$ is equivalent to $(g,V)$ if there is an open set $W\subset U\cap V$ containing $p$ such that $f=g$ when restricted to $W$. The equivalence class of $(f,U)$ is called the **germ** of $f$ at $p$. We write $C_p^\infty(\R^n)$ for the set of all germs of $C^\infty$ functions on $\R^n$ at $p$.

**Example:** The functions
$$
	f(x) = \frac{1}{1-x}
$$
with domain $\R\setminus\{1\}$ and
$$
	g(x) = 1+x+x^2+x^3+\dots
$$
with domain $(-1,1)$ have the same germ at any point $p\in(-1,1)$ because $g$ is the geometric series
$$
	\sum_{n=1}^\infty x^{n-1} 
$$
which converges to $f(x)$ for all $|x|<1$.

> [!def] Algebra
> An **algebra** over a field $k$ is a vector space $A$ over $k$ with a multiplication map $\mu:A\times A\rightarrow A$ such that for all $a,b,c\in A$ and $r\in k$
> 1. $(a\cdot b)\cdot c = a\cdot(b\cdot c)$
> 2. $(a+b)\cdot c = a\cdot c+b\cdot c$ and $a\cdot(b+c)=a\cdot b+a\cdot c$
> 3. $r(a\cdot b)=(ra)\cdot b=a\cdot(rb)$.

Note that an algebra has three operations, addition, multiplication from the ring, and multiplication from the vector space.

> [!def] Linear map
> A map $L:V\rightarrow W$ between vector spaces over a field $k$ is called a **linear map** if for any $r\in k$ and $u,v\in V$,
> 1. $L(u+v) = L(u) + L(v)$
> 2. $L(rv)=rL(v)$

If $A$ and $A'$ are algebras over a field $k$, then an **algebra homomorphism** is a linear map $L:A\rightarrow A'$ such that $L(ab)=L(a)L(b)$ for all $a,b\in A$.

> [!prp]
> $C_p^\infty$ is an algebra over $\R$.

**Proof:** Let $(f,U),(g,V),(h,W)\in C_p^\infty$ and $s\in\R$. Define $(f,U)\cdot(g,V)$ to be the germ $(fg,U\cap V)$ and $(f,U)+(g,V)$ to be the germ $(f+g,U\cap V)$. Notice that
$$
	((f,U)\cdot(g,V))\cdot(h,W)=(fg,U\cap V)\cdot(h,W)=(fgh,U\cap V\cap W)
$$
and 
$$
	(f,U)\cdot((g,V)\cdot(h,W)) = (f,U)\cdot(gh,V\cap W)=(fgh,U\cap V\cap W).
$$
Next, notice that
$$
	((f,U)+(g,V))\cdot(h,W) = (f+g,U\cap V)\cdot(h,W) = (fh+gh,U\cap V\cap W) = (f,U)\cdot(h,W)+(g,V)\cdot(h,W)
$$
and
$$
	(f,U)\cdot((g,V)+(h,W)) = (f,U)\cdot(g+h,V\cap W)=(fg+fh,U\cap V\cap W) = (f,U)\cdot(g,V) + (f,U)\cdot(h,W).
$$
Finally, we define $s(f,U)$ to be the germ $(sf,U)$. Then one may see that 
$$
	s((f,U)\cdot(g,V)) = (s(f,U))\cdot(g,V) = (f,U)\cdot(s(g,V)).
$$
Therefore, $C_p^\infty$ is an algebra over $\R$.
<p style='text-align:right'>Q.E.D.</p>

# Derivations at a Point

Notice that for each tangent vector $v\in T_p(\R^n)$, the directional derivative at $p$ gives a map of real vector spaces
$$
	D_v:C_p^\infty\rightarrow\R
$$
which maps germs $(f,U)$ to their directional derivative $D_vf$ at $p\in U$. One may show that $D_v$ is $\R$-linear and satisfies the Leibniz rule
$$
	D_v(fg) = (D_vf)g(p) + f(p)D_vg.
$$
Indeed, notice that 
$$
\begin{align*}
	D_v(f+g) = \sum_{i=1}^nv^i\frac{\partial(f+g)}{\partial x^i}(p) = \sum_{i=1}^nv^i\frac{\partial f}{\partial x^i}(p) + \sum_{i=1}^nv^i\frac{\partial g}{\partial x^i}(p) = D_vf + D_vg
\end{align*}
$$
and
$$
	D_v(rf) = \sum_{i=1}^nv^i\frac{\partial rf}{\partial x_i}(p) = r\sum_{i=1}^nv^i\frac{\partial f}{\partial x^i}(p) = rD_vf.
$$
As for Leibniz's rule,
$$
	D_v(fg) = \sum_{i=1}^nv^i\frac{\partial(fg)}{\partial x^i}(p) = \sum_{i=1}^nv^i\frac{\partial f}{\partial x^i}(p)g(p) + \sum_{i=1}^nv^i\frac{\partial g}{\partial x^i}(p)f(p) = (D_vf)g(p) + f(p)D_vg.
$$

In general, any linear map $D:C_p^\infty\rightarrow\R$ satisfying Leibniz rule is called a **derivation** at $p$. We denote the set of all derivations at $p$ by $\cal{D}_p(\R^n)$. It follows that all directional derivatives at a point $p$ are derivations at $p$ by the map
$$
	v\in T_p(\R^n)\mapsto D_v=\sum v^i\frac{\partial}{\partial x^i}\Bigg|_p. \tag{1}
$$
> [!lemma]
> If $D$ is a point-derivation of $C_p^\infty$, then $D(c)=0$ for any constant function $c$.

**Proof:** Notice that by $\R$-linearity, $D(c)=cD(1)$. So it suffices to show that $D(1)=0$. By making use of Leibniz's rule,
$$
	D(1) = D(1\cdot 1) = D(1)1 + 1D(1) = 2D(1)
$$
which holds if and only if $D(1)=0$. 
<p style='text-align:right'>Q.E.D.</p>

> [!thm] 
> The linear map $\phi:T_p(\R^n)\rightarrow\cal{D}_p(\R^n)$ defined in (1) is an isomorphism of vector spaces.

**Proof:** We need to show that $\phi$ is a bijection and $\phi$ is linear. Suppose $\phi(v)=D_v=0$. Then it follows that 
$$
	0 = \sum v^i\frac{\partial x^j}{\partial x^i}(p) = \sum v^i\delta_i^j = v^j
$$
where $\delta_i^j$ is the Kronecker delta. This holds for all $j$, hence $v=0$.

Now let $D\in D_p(\R^n)$ be a point-derivation at $p$. Consider a germ $(f,V)$ in $C_p^\infty$ such that $V$ is star-shaped. By Taylor's thoerem,
$$
	f(x) = f(p) + \sum_{i=1}^n(x^i-p^i)g_i(x),~\text{ where } g_i(p) = \frac{\partial f}{\partial x^i}(p).
$$
Notice that by applying $D$ to both sides
$$
\begin{align*}
	Df(x) &= D(f(p)) + \sum D((x^i-p^i)g_i(x)) \\
	&= \sum D(x^i)g_i(p) + \sum (p^i-p^i)Dg_i(x) \\
	&= \sum Dx^i\frac{\partial f}{\partial x^i}(p).
\end{align*}
$$
Thus, $D=D_v$ where $v=\langle Dx^1,\dots,Dx^n\rangle$ for all $f\in C_p^\infty$. Hence $\phi$ is surjective.

Linearity is trivial.
<p style='text-align:right'>Q.E.D.</p>

As a result, the standard basis $e_1,\dots,e_n$ for $T_p(\R^n)$ corresponds to the partial derivatives $\partial/\partial x^1|_p,\dots,\partial/\partial x^n|_p$.

# Vector Fields


> [!def]
> A **vector field** $X$ on a subset $U$ of $\R^n$ is a function that assigns to each point $p\in U$ a tangent vector $X_p\in T_p(\R^n)$.

The vector $X_p\in T_p(\R^n)$ can be expressed as a linear combination
$$
	X_p = \sum a^i(p)\frac{\partial}{\partial x^i}\Bigg|_p.
$$
Thus, one may view the vector space $X$ as the linear combination 
$$
	X = \sum a^i\frac{\partial}{\partial x^i}
$$
where $a^i:U\rightarrow\R$ are some functions. We say $X$ is $C^\infty$ if each coefficient function $a^i$ is $C^\infty$. One may identify vector fields on $U$ with column vectors:
$$
	X = \sum a^i\frac{\partial}{\partial x^i}\longleftrightarrow \begin{bmatrix} a^1\\\vdots\\ a^n\end{bmatrix}.
$$
**Example:** On $\R^2\setminus\{0\}$ let $p=(x,y)$. Then
$$
	X = \frac{-y}{\sqrt{x^2+y^2}}\frac{\partial}{\partial x} + \frac{x}{\sqrt{x^2+y^2}}\frac{\partial}{\partial y}
$$
is the vector field below:

![[Screenshot from 2024-07-23 15-55-22.png#invert | center]]

One may define multiplication of vector fields by functions on $U$ pointwise:
$$
	(fX)_p = f(p)X_p,~p\in U.
$$
Notice that if $X$ is a $C^\infty$ vector field, and $f$ is a $C^\infty$ function on $U$, then $fX$ is a $C^\infty$ vector field on $U$. Hence, the set of all $C^\infty$ vector fields on $U$, denoted $\mathfrak{X}(U)$, is a module over the ring $C^\infty(U$).

> [!def]
> If $R$ is a commutative ring with identity, then a (left) **$R$-module** is an abelian group $A$ with a scalar multiplication map $\mu:R\times A\rightarrow A$ such that for all $r,s\in R$ and $a,b\in A$
> 1. $(rs)a=r(sa)$
> 2. $1a=a$
> 3. $(r+s)a=ra+sa$, $r(a+b)=ra+rb$.

> [!def]
> Let $A$ and $A'$ be $R$-modules. An **$R$-module homomorphism** from $A$ to $A'$ is a map $f:A\rightarrow A'$ such that
> 1. $f(a+b)=f(a)+f(b)$
> 2. $f(ra) = rf(a)$.

# Vector Fields as Derivations

Let $X$ be a $C^\infty$ vector field on $U\subseteq\R^n$ and $f:U\rightarrow\R$ a $C^\infty$ function. Then we define a new function $Xf:U\rightarrow\R$ by
$$
	(Xf)(p) = X_pf = \sum a^i(p)\frac{\partial f}{\partial x^i}(p).
$$
Thus, a $C^\infty$ vector field gives rise to an $\R$-linear map
$$
\begin{align*}
	C^\infty(U) \rightarrow C^\infty(U) \\
	f \mapsto Xf = \sum a^i\frac{\partial f}{\partial x^i}
\end{align*}
$$
We claim this map is a derivation.

> [!prp]
> If $X$ is a $C^\infty$ vector field and $f$ and $g$ are $C^\infty$ functions on $U\subseteq\R^n$, then $X(fg)$ satisfies Leibniz rule
> $$
> 	X(fg) = (Xf)g + fXg.
> $$

**Proof:** Notice that for $p\in U$
$$
\begin{align*}
	X_p(fg) &= \sum a^i(p)\frac{\partial(fg)}{\partial x^i}(p) \\
	&= \sum a^i(p)\frac{\partial f}{\partial x^i}(p)g(p) + \sum a^i(p)f(p)\frac{\partial g}{\partial x^i}(p) \\
	&= (X_pf)g(p) + f(p)X_pf.
\end{align*}
$$
This holds for every point $p\in U$.
<p style='text-align:right'>Q.E.D.</p>

# Problems

##### 2.1. Vector fields
Let $X$ be the vector field $x\partial/\partial x+y\partial/\partial y$ and $f(x,y,z)=x^2+y^2+z^2$ on $\R^3$. Compute $Xf$.

**Answer:** 
$$
\begin{align*}
	Xf &= x\frac{\partial f}{\partial x} + y\frac{\partial f}{\partial y} + 0\frac{\partial f}{\partial z} = 2x^2 + 2y^2
\end{align*}
$$

##### 2.3. Vector space structure on derivations at a point
Let $D$ and $D'$ be derivations at $p$ in $\R^n$, and $c\in\R$. Prove that the sum $D+D'$ and the scalar multiple $cD$ are derivations at $p$.

**Answer:** Clearly they are linear, so we simply need to show they satisfy Leibniz's rule. Notice that 
$$
\begin{align*}
	(D+D')(fg) &= D(fg) + D'(fg) \\
	&= (Df)g + fDg + (D'f)g + fD'g \\
	&= ((D+D')f)g + f(D+D')g.
\end{align*}
$$
Furthermore,
$$
\begin{align*}
	(cD)(fg) &= cD(fg) = c((Df)g + fDg) = ((cD)f)g + f(cD)g.
\end{align*}
$$

